---
title: "[ 딥러닝 ]  LSTM & GRU"
date: 2022-02-13
excerpt: "LSTM과 GRU에 대해 알아봅시다."
categories: 
    - Deep Learning
toc: true
toc_sticky: true
---


## Long Short Term Memory (LSTM)

![LSTM 구조.jpg](Recurrent%20Neural%20Networks%2070865ebb9fcc4a51af38e5a11e7b64e6/LSTM_%EA%B5%AC%EC%A1%B0.jpg)

![Untitled](Recurrent%20Neural%20Networks%2070865ebb9fcc4a51af38e5a11e7b64e6/Untitled%203.png)

![LSTM cell 구조.png](Recurrent%20Neural%20Networks%2070865ebb9fcc4a51af38e5a11e7b64e6/LSTM_cell_%EA%B5%AC%EC%A1%B0.png)

- RNN의 hidden state에 **cell state를 추가**한 구조
    - cell state : **유용한 정보만 저장**한다.
- 입력
    - input
    - previous cell state
    - previous hidden state (output)
- 출력
    - output (혹은 hidden state)
    - next cell state
    - next hidden state
    

### Forget Gate

![Untitled](Recurrent%20Neural%20Networks%2070865ebb9fcc4a51af38e5a11e7b64e6/Untitled%204.png)

$$
f_t 
= \sigma(W_f \cdot[h_{t-1},x_t])
= \sigma(W_{xh\_f}x_t+W_{hh\_f}h_{t-1})
$$

- $\sigma$ : sigmoid function
- 어떤 정보를 **cell state에서 제거**할 것인가?

### Input Gate

![Untitled](Recurrent%20Neural%20Networks%2070865ebb9fcc4a51af38e5a11e7b64e6/Untitled%205.png)

$$
\begin{align*}
&i_t \odot \tilde{C}_t
\\
& i_t 
= \sigma(W_i \cdot[h_{t-1},x_t])
= \sigma(W_{xh\_i}x_t+W_{hh\_i}h_{t-1})
\\
& \tilde{C}_t 
= tanh(W_C \cdot[h_{t-1},x_t])
= tanh(W_{xh\_\tilde{C}}x_t+W_{hh\_\tilde{C}}h_{t-1})

\end{align*}
$$

- 어떤 정보를 **cell state에서 더해줄 것**인가?
- $i_t$ : 현재 정보를 cell state에 올릴지 말지 결정
- $\tilde{C}_t$ : 현재 정보와 이전 output으로 얻어지는 cell state candidate

### Update Cell

![Untitled](Recurrent%20Neural%20Networks%2070865ebb9fcc4a51af38e5a11e7b64e6/Untitled%206.png)

$$
c_t=f_t \odot c_{t-1}+i_t \odot \tilde{C}_t
$$

- forget gate와 input gate를 통과한 결과들을 가지고 cell state를 업데이트한다.
- cell state에 대한 정보는 cell 밖으로 나가지 않는다.

### Output Gate

![Untitled](Recurrent%20Neural%20Networks%2070865ebb9fcc4a51af38e5a11e7b64e6/Untitled%207.png)

$$
\begin{align*}
& o_t = \sigma(W_{xh\_o}x_t+W_{hh\_o}h_{t-1})
\\
& h_t = o_t \odot tanh(c_t)
\end{align*}
$$

- 어떤 정보를 **읽을 것**인가?
- 업데이트 된 cell state를 기반으로 결과를 출력시킨다.

## Gated Recurrent Unit (GRU)

![Untitled](Recurrent%20Neural%20Networks%2070865ebb9fcc4a51af38e5a11e7b64e6/Untitled%208.png)

- cell state는 없고 hidden state만 존재한다.
- LSTM보다 학습할 weight 수가 적다.

### Reset Gate

$$
\begin{align*}
r_t 
&= \sigma(W_r \cdot [h_{t-1},x_t])
\\
&= \sigma(W_{hh\_r}h_{t-1}+W_{xh\_r}x_t)
\end{align*}
$$

- 과거의 정보를 적당히 reset한다.

### Update Gate

$$
\begin{align*}
z_t 
& = \sigma(W_z \cdot [h_{t-1},x_t])
\\
& = \sigma(W_{hh\_z}h_{t-1}+W_{xh\_z}x_t)
\end{align*}
$$

- LSTM의 forget gate + input gate
- 과거($1-z_t$)와 현재 정보($z_t$)의 up-to-date 비율을 결정한다.

### Candidate

$$
\begin{align*}
\tilde{h}_t 
& = tanh(W \cdot [r_t*h_{t-1},x_t])
\\
& = tanh(Wr_t*h_{t-1}+W_{xh\_\tilde{h}}x_t)
\end{align*}
$$

- 현재의 정보 candidate 계산

### hidden state 계산

$$
\begin{align*}
h_t = (1-z_t)*h_{t-1}+z_t*\tilde{h}_t
\end{align*}
$$

- 현재의 hidden state 계산