---
title: "[ 딥러닝 ]  GRU"
date: 2022-02-13
excerpt: "GRU에 대해 알아봅시다."
categories: 
    - DeepLearning
toc: true
toc_sticky: true
---


## Gated Recurrent Unit (GRU)

![Untitled](/assets/images/posts/deep_learning/gru/1.png){:width="400"}

- cell state는 없고 hidden state만 존재한다.
- LSTM보다 학습할 weight 수가 적다.

<br/>

## Reset Gate

$$
\begin{align*}
r_t 
&= \sigma(W_r \cdot [h_{t-1},x_t])
\\
&= \sigma(W_{hh\_r}h_{t-1}+W_{xh\_r}x_t)
\end{align*}
$$

- 과거의 정보를 적당히 reset한다.

<br/>

## Update Gate

$$
\begin{align*}
z_t 
& = \sigma(W_z \cdot [h_{t-1},x_t])
\\
& = \sigma(W_{hh\_z}h_{t-1}+W_{xh\_z}x_t)
\end{align*}
$$

- LSTM의 forget gate + input gate
- 과거($1-z_t$)와 현재 정보($z_t$)의 up-to-date 비율을 결정한다.

<br/>

## Candidate

$$
\begin{align*}
\tilde{h}_t 
& = tanh(W \cdot [r_t*h_{t-1},x_t])
\\
& = tanh(Wr_t*h_{t-1}+W_{xh\_\tilde{h}}x_t)
\end{align*}
$$

- 현재의 정보 candidate 계산

<br/>

## hidden state 계산

$$
\begin{align*}
h_t = (1-z_t)*h_{t-1}+z_t*\tilde{h}_t
\end{align*}
$$

- 현재의 hidden state 계산